{"paragraphs":[{"text":"import org.apache.spark.ml.feature.MinMaxScaler\nimport org.apache.spark.ml.feature.MinMaxScalerModel\nimport org.apache.spark.ml.feature.StandardScaler\nimport org.apache.spark.ml.feature.StandardScalerModel\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\n\nval c_prefix = \"/home/duclv/data/china/\"\nval cschema = StructType(\n    StructField(\"day\", StringType, true) ::\n    StructField(\"hour\", IntegerType, true) ::\n    StructField(\"avg\", DoubleType, true) ::\n    StructField(\"conc\", DoubleType, true) ::\n    StructField(\"city\", IntegerType, true) :: Nil\n)\nval datap = spark.read.option(\"header\", \"true\").csv(\"/home/duclv/data/sensorParser.csv\")\nvar full_date = spark.read.option(\"header\", \"false\").schema(StructType(StructField(\"day\", StringType, true) :: Nil)).csv(c_prefix + \"check_date.txt\")\nval pm25range = Array((0, 50),(51,100),(101,150),(151,200),(201,300),(301,400),(401,500))\nval pm25text = Array(\"Good\",\"Moderate\",\"Unhealthy for Sensitive Groups\",\"Unhealthy\",\"Very Unhealthy\",\"Hazardous\",\"Hazardous\")\nval (x1, x2) = (128.38552f, 128.763315f) // lng\nval (y1, y2) = (35.607048f, 36.017445f) // lat\nval features_common = Array(\"pm2_5_value\", \"pm10_value\",\"voc_value\",\"so2_value\",\"no2_value\",\"co_value\",\"mcp_value\", \"hour\")\nval features_weather = Array(\"temp\", \"prec\", \"wind_sp\")\nval features_pressure = Array(\"pres_value\")\nval cn_features = Array(\"avg\")\nval dg_schema = StructType( \n    StructField(\"id\", StringType, true) ::\n    StructField(\"gateway_id\", StringType, true) ::\n    StructField(\"node_id\", StringType, true) ::\n    StructField(\"timestamp\", StringType, true) ::\n    StructField(\"total_index\", IntegerType, true) ::\n    StructField(\"total_cai\", IntegerType, true) ::\n    StructField(\"total_percent\", IntegerType, true) ::\n    StructField(\"so2_index\", IntegerType, true) ::\n    StructField(\"so2_cai\", IntegerType, true) ::\n    StructField(\"so2_percent\", IntegerType, true) ::\n    StructField(\"so2_value\", DoubleType, true) ::\n    StructField(\"no2_index\", IntegerType, true) ::\n    StructField(\"no2_cai\", IntegerType, true) ::\n    StructField(\"no2_percent\", IntegerType, true) ::\n    StructField(\"no2_value\", DoubleType, true) ::\n    StructField(\"co_index\", IntegerType, true) ::\n    StructField(\"co_cai\", IntegerType, true) ::\n    StructField(\"co_percent\", IntegerType, true) ::\n    StructField(\"co_value\", DoubleType, true) ::\n    StructField(\"pm2_5_index\", IntegerType, true) ::\n    StructField(\"pm2_5_cai\", IntegerType, true) ::\n    StructField(\"pm2_5_percent\", IntegerType, true) ::\n    StructField(\"pm2_5_value\", DoubleType, true) ::\n    StructField(\"pm10_index\", IntegerType, true) ::\n    StructField(\"pm10_cai\", IntegerType, true) ::\n    StructField(\"pm10_percent\", IntegerType, true) ::\n    StructField(\"pm10_value\", DoubleType, true) ::\n    StructField(\"co2_value\", DoubleType, true) ::\n    StructField(\"voc_index\", IntegerType, true) ::\n    StructField(\"voc_value\", DoubleType, true) ::\n    StructField(\"temp_index\", IntegerType, true) ::\n    StructField(\"temp_value\", DoubleType, true) ::\n    StructField(\"hum_index\", IntegerType, true) ::\n    StructField(\"hum_value\", DoubleType, true) ::\n    StructField(\"pres_value\", DoubleType, true) ::\n    StructField(\"mcp_index\", IntegerType, true) ::\n    StructField(\"mcp_cai\", IntegerType, true) ::\n    StructField(\"mcp_percent\", IntegerType, true) ::\n    StructField(\"mcp_value\", DoubleType, true) ::\n    StructField(\"vbr_value\", StringType, true) ::\n    StructField(\"lat\", DoubleType, true) ::\n    StructField(\"lng\", DoubleType, true) ::\n    StructField(\"spd\", IntegerType, true) ::\n    StructField(\"areaMapKeyStr\", StringType, true) :: Nil\n)","dateUpdated":"2018-03-09T20:45:51+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1516415651271_637128110","id":"20180120-113411_1577120230","result":{"code":"SUCCESS","type":"TEXT","msg":"\nimport org.apache.spark.ml.feature.MinMaxScaler\n\nimport org.apache.spark.ml.feature.MinMaxScalerModel\n\nimport org.apache.spark.ml.feature.StandardScaler\n\nimport org.apache.spark.ml.feature.StandardScalerModel\n\nimport org.apache.spark.ml.feature.VectorAssembler\n\nimport org.apache.spark.mllib.linalg.Vectors\n\nimport org.apache.spark.sql.DataFrame\n\nimport org.apache.spark.sql._\n\nimport org.apache.spark.sql.functions._\n\nimport org.apache.spark.sql.types._\n\nc_prefix: String = /home/duclv/data/china/\n\ncschema: org.apache.spark.sql.types.StructType = StructType(StructField(day,StringType,true), StructField(hour,IntegerType,true), StructField(avg,DoubleType,true), StructField(conc,DoubleType,true), StructField(city,IntegerType,true))\n\ndatap: org.apache.spark.sql.DataFrame = [id: string, gateway_id: string ... 42 more fields]\n\nfull_date: org.apache.spark.sql.DataFrame = [day: string]\n\npm25range: Array[(Int, Int)] = Array((0,50), (51,100), (101,150), (151,200), (201,300), (301,400), (401,500))\n\npm25text: Array[String] = Array(Good, Moderate, Unhealthy for Sensitive Groups, Unhealthy, Very Unhealthy, Hazardous, Hazardous)\n\n\nx1: Float = 128.38551\nx2: Float = 128.76332\n\n\ny1: Float = 35.607048\ny2: Float = 36.017445\n\nfeatures_common: Array[String] = Array(pm2_5_value, pm10_value, voc_value, so2_value, no2_value, co_value, mcp_value, hour)\n\nfeatures_weather: Array[String] = Array(temp, prec, wind_sp)\n\nfeatures_pressure: Array[String] = Array(pres_value)\n\ncn_features: Array[String] = Array(avg)\ndg_schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,true), StructField(gateway_id,StringType,true), StructField(node_id,StringType,true), StructField(timestamp,StringType,true), StructField(total_index,IntegerType,true), StructField(total_cai,IntegerType,true), StructField(total_percent,IntegerType,true), StructField(so2_index,IntegerType,true), StructField(so2_cai,IntegerType,true), StructField(so2_percent,IntegerType,true), StructField(so2_value,DoubleType,true), StructField(no2_index,IntegerType,true), StructField(no2_cai,IntegerType,true), StructField(no2_percent,IntegerType,true), StructField(no2_value,DoubleType,true), StructField(co_index,IntegerType,true), StructField(co_cai,IntegerType,true), StructField(co_percent,IntegerType,true), StructFi..."},"dateCreated":"2018-01-20T11:34:11+0900","dateStarted":"2018-03-04T17:20:21+0900","dateFinished":"2018-03-04T17:20:29+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:231"},{"text":"// normalize\ndef proc_pm25(v: Short): Float = {\n    var x = 0.0f\n    if (v >= 0){\n        x = v.toFloat / 500\n    }\n    return x\n}\n\nval normalize_pm25 = udf(proc_pm25 _)\n\nval process_latlng = (x: Float) => x / 180\nval normalize_latlng = udf(process_latlng)\n\n// def agg_latlng(lat: Float, lng: Float) : Short = {\n//     // lng is x, lat is y\n//     var (gx, gy) = (1, 4)\n//     var sx = (x2 - x1) / gx\n//     var sy = (y2 - y1) / gy\n//     var ix = math.ceil((lng - x1) / sx)\n//     ix = if(ix <= 0) 1 else if(ix > 0 && ix <= gx) ix else gx\n//     var iy = math.ceil((lat - y1) / sy)\n//     iy = if(iy <= 0) 1 else if(iy > 0 && iy <= gy) iy else gy\n//     return ((iy - 1) * gx + ix).toShort\n// }\n// val udf_agg_latlng = udf(agg_latlng _)\n// 1050 is maximum pressure on earth\nval pr_norm_pres = (x : Float) => x / 1050\n\nval process_mcp = (x : Float) => if(x < 0) 0 else x\nval udf_mcp = udf(process_mcp)\n\nval process_pm10 = (x : Float) => if(x < 0 || x > 500) 0 else x\nval udf_pm10 = udf(process_pm10)\n\nval proc_time = (x:  String) => ((if(x.toInt < 10) \"0\" + x else x) + \":00:00\")\nval udf_time = udf(proc_time)\n\ndef merge_datetime(date: String, time: String) : String = {\n    // lng is x, lat is y\n   var t = proc_time(time)\n   return date + \" \" + t\n}\nval udf_datetime = udf(merge_datetime _)\n// generate a sequence id for dataframe\nval get_weekend = (x: String) => if(x == \"Sat\" || x == \"Sun\") 1 else 0\nval udf_weekend = udf(get_weekend)\n\n// pre-process transportation data\ndef get_core_df(dframe: DataFrame, is_order: Boolean = false) : DataFrame = {\n    var dataf = dframe.filter(col(\"gateway_id\").equalTo(\"SERVER\") \n                  && $\"pm2_5_value\" >= 0 && $\"pm2_5_value\" <= 500 \n                  && $\"timestamp\" >= \"2017-06-02 00:00:00\" \n                //   && $\"timestamp\" < \"2017-12-25 00:00:00\"\n                  && $\"lng\" >= x1 && $\"lng\" <= x2 && $\"lat\" >= y1 && $\"lat\" <= y2)\n        .select($\"timestamp\", $\"lat\", $\"lng\", $\"pm2_5_value\".cast(\"short\"), udf_pm10($\"pm10_value\").alias(\"pm10_value\"),\n            udf_mcp($\"voc_value\").alias(\"voc_value\"), udf_mcp($\"so2_value\").alias(\"so2_value\"), udf_mcp($\"no2_value\").alias(\"no2_value\"),\n            udf_mcp($\"co_value\").alias(\"co_value\"), udf_mcp($\"mcp_value\").alias(\"mcp_value\"))\n        .na.fill(0.0, Array(\"lat\",\"lng\",\"voc_value\",\"so2_value\",\"no2_value\",\"co_value\",\"pres_value\",\"mcp_value\"))\n        .na.fill(0, Array(\"pm2_5_value\",\"pm10_value\"))\n        .withColumn(\"timestamp\", (((unix_timestamp($\"timestamp\", \"yyyy-MM-dd HH:mm:ss\")) /60).cast(\"long\") * 60).cast(\"timestamp\"))\n        .groupBy(\"timestamp\")\n        .agg(avg($\"lat\").alias(\"lat\"), avg($\"lng\").alias(\"lng\"),   \n            avg($\"pm2_5_value\").alias(\"pm2_5_value\"), avg($\"pm10_value\").alias(\"pm10_value\"),\n            avg($\"voc_value\").alias(\"voc_value\"), avg($\"so2_value\").alias(\"so2_value\"),\n            avg($\"no2_value\").alias(\"no2_value\"), avg($\"co_value\").alias(\"co_value\"),\n            avg($\"mcp_value\").alias(\"mcp_value\"))\n        .withColumn(\"shift\", (((unix_timestamp($\"timestamp\", \"yyyy-MM-dd HH:mm:ss\")) / 1800).cast(\"long\") * 1800).cast(\"timestamp\"))\n        .withColumn(\"hourly\", (((unix_timestamp($\"timestamp\", \"yyyy-MM-dd HH:mm:ss\")) / 3600).cast(\"long\") * 3600).cast(\"timestamp\"))\n        .withColumn(\"hour\", hour($\"hourly\"))\n        .withColumn(\"dow\", udf_weekend(date_format($\"hourly\", \"E\")))\n        .withColumn(\"date\", to_date($\"timestamp\"))\n    if(is_order){\n        dataf = dataf.orderBy(asc(\"timestamp\"))\n    }\n    return dataf\n}\ndef init_weather(data: DataFrame) : DataFrame = {\n    var wdf_frame = data.select($\"code\", udf_time($\"hourly\").cast(\"timestamp\").alias(\"hourly\"), $\"temp\", $\"prec\", $\"wind_sp\", $\"wind_dir\")\n                        .groupBy($\"hourly\")\n                        .agg(avg($\"temp\").alias(\"temp\"), avg($\"prec\").alias(\"prec\"), avg($\"wind_sp\").alias(\"wind_sp\"), first($\"wind_dir\").alias(\"wind_dir\"), first($\"code\").alias(\"code\"))\n                        .orderBy(asc(\"hourly\"), asc(\"code\"))\n    return wdf_frame\n}\ndef generate_seq(df_data: DataFrame) : DataFrame = {\n    var rdd_fk = df_data.rdd.zipWithIndex()\n                 .map(index => Row.fromSeq(index._2.toString +: index._1.toSeq))\n    var stk = StructType(Seq(StructField(\"seq\", StringType, true)).++(df_data.schema.fields))\n    var res_df = spark.createDataFrame(rdd_fk, stk)    \n    return res_df\n}\n// Array(\"pm2_5_value\", \"pm10_value\", \"lat\", \"lng\", \"voc_value\",\"so2_value\",\"no2_value\",\"co_value\",\"mcp_value\")\ndef normalize_vector(df_data : DataFrame, cols: Array[String], norm: String = \"norm_feature\", orb: String = \"timestamp\") : (DataFrame, MinMaxScalerModel) = {\n    var vectorAssembler = new VectorAssembler().setInputCols(cols).setOutputCol(\"features_norm\")\n    var mxm = new MinMaxScaler().setInputCol(\"features_norm\").setOutputCol(\"features\")\n    \n    var output = vectorAssembler.transform(df_data)\n    var mxm_scaler = mxm.fit(output)\n    var dataset = mxm_scaler.transform(output)\n    // dataset.show()\n    var norm_data = dataset.select($\"features\".alias(norm), dataset(orb)).orderBy(orb)\n    // norm_data.show()\n    return (norm_data, mxm_scaler)\n}\ndef std_normalize_vector(df_data : DataFrame, cols: Array[String], norm: String = \"norm_feature\", orb: String = \"timestamp\") : (DataFrame, StandardScalerModel) = {\n    var vectorAssembler = new VectorAssembler().setInputCols(cols).setOutputCol(\"features_norm\")\n    var std_scaler = new StandardScaler().setInputCol(\"features_norm\")\n                                  .setOutputCol(\"features\")\n                                  .setWithStd(true)\n                                  .setWithMean(true)\n    \n    var output = vectorAssembler.transform(df_data)\n    var std = std_scaler.fit(output)\n    var dataset = std.transform(output)\n    var norm_data = dataset.select(col(orb), $\"features\".alias(norm)).orderBy(orb)\n    return (norm_data, std)\n}\ndef normalize_vector_to_trainset(df_data : DataFrame, df_data1 : DataFrame, cols: Array[String], norm: String = \"norm_feature\", orb: String = \"timestamp\") : DataFrame = {\n    var vectorAssembler = new VectorAssembler().setInputCols(cols).setOutputCol(\"features_norm\")\n    var std_scaler = new StandardScaler().setInputCol(\"features_norm\")\n                                  .setOutputCol(\"features\")\n                                  .setWithStd(true)\n                                  .setWithMean(true)\n    \n    var output = vectorAssembler.transform(df_data)\n    var std = std_scaler.fit(output)\n    var output1 = vectorAssembler.transform(df_data1)\n    var dataset = std.transform(output1)\n    var norm_data = dataset.select(col(orb), $\"features\".alias(norm)).orderBy(orb)\n    return norm_data\n}\ndef normalize_vector_to_trainset(std : StandardScalerModel, df_data1 : DataFrame, cols: Array[String], norm: String = \"norm_feature\", orb: String = \"timestamp\") : DataFrame = {\n    var vectorAssembler = new VectorAssembler().setInputCols(cols).setOutputCol(\"features_norm\")\n    var output1 = vectorAssembler.transform(df_data1)\n    var dataset = std.transform(output1)\n    var norm_data = dataset.select(col(orb), $\"features\".alias(norm)).orderBy(orb) \n    return norm_data\n}\n// process pressure information\ndef init_pressure(pdata: DataFrame): DataFrame ={\n    var pdf_frame = pdata.withColumn(\"date\", to_date(unix_timestamp($\"date\", \"yyyy-MM-dd\").cast(\"timestamp\")))\n                         .select($\"date\", $\"pres_value\")\n    return pdf_frame;\n}\ndef merge_weather(pnorm: DataFrame, pdf: DataFrame): DataFrame = {\n    var pnorm_s_ = generate_seq(pnorm)\n    var pdf_s_ = generate_seq(pdf.orderBy(\"hourly\"))\n    var final_frame = pdf_s_.join(pnorm_s_, \"seq\")\n                            .select(pdf_s_(\"hourly\"), $\"wind_dir\", $\"weather_feature\")\n    return final_frame\n}\ndef merge_pressure(wnorm: DataFrame, wresult: DataFrame): DataFrame = {\n    var wnorm_s_ = generate_seq(wnorm)\n    var wdf_s_ = generate_seq(wresult.orderBy(\"date\"))\n    var final_w_frame = wdf_s_.join(wnorm_s_, \"seq\")\n                              .select(wdf_s_(\"date\"), $\"pres_feature\")\n    return final_w_frame\n}\ndef merge_china(vectors: DataFrame, data: DataFrame, feature_alias: String = \"china\") : DataFrame = {\n    var v_ = generate_seq(vectors)\n    var d_ = generate_seq(data.orderBy(\"timestamp\"))\n    var final_w_frame = d_.join(v_, \"seq\")\n                              .select(d_(\"timestamp\"), $\"china\".alias(feature_alias))\n    return final_w_frame\n}\n// merge weather and pressure to original data\ndef generate_data_file(df_ : DataFrame, norm_data_ : DataFrame, wnorm_ : DataFrame, wresult_ : DataFrame, pnorm_ : DataFrame, pdf_ : DataFrame, \n                       b_vectors: DataFrame, beijing: DataFrame, s_vectors: DataFrame, shenyang: DataFrame): DataFrame = {\n    var final_p_fr = merge_pressure(pnorm_, pdf_)\n    var final_w_fr = merge_weather(wnorm_, wresult_)\n    var seq_dat_fr = generate_seq(df_.orderBy(\"timestamp\"))\n    var norm_dg_data = generate_seq(norm_data_)\n    var bej = merge_china(b_vectors, beijing, \"beijing\")\n    var she = merge_china(s_vectors, shenyang, \"shenyang\")\n    var fn_ = seq_dat_fr\n                .join(norm_dg_data, \"seq\")\n                .join(final_p_fr, \"date\")\n                .join(final_w_fr, \"hourly\")\n                .join(bej, bej(\"timestamp\") === seq_dat_fr(\"hourly\"), \"left_outer\")\n                .join(she, she(\"timestamp\") === seq_dat_fr(\"hourly\"), \"left_outer\")\n                .groupBy($\"shift\").agg(\n                            collect_list(\"common_feature\"),\n                            collect_list(\"weather_feature\"), \n                            collect_list(\"pres_feature\"),\n                            collect_list(\"beijing\"), \n                            collect_list(\"shenyang\"), \n                            collect_list(\"wind_dir\"))\n                .orderBy(asc(\"shift\"))    \n    return fn_\n}\n// merge weather and pressure to original data\ndef generate_data_file_s(df_ : DataFrame, norm_data_ : DataFrame, wnorm_ : DataFrame, wresult_ : DataFrame): DataFrame = {\n    var final_w_fr = merge_weather(wnorm_, wresult_)\n    var seq_dat_fr = generate_seq(df_.orderBy(\"timestamp\"))\n    var norm_dg_data = generate_seq(norm_data_)\n    var fn_ = seq_dat_fr\n                .join(norm_dg_data, \"seq\")\n                .join(final_w_fr, \"hourly\")\n                .groupBy($\"shift\").agg(\n                            collect_list(\"common_feature\"),\n                            collect_list(\"weather_feature\"), \n                            collect_list(\"wind_dir\"))\n                .orderBy(asc(\"shift\"))    \n    return fn_\n}\n// merge weather and pressure to original data\ndef get_labels(seq_dat_fr : DataFrame, wresult_ : DataFrame, pdf_ : DataFrame): DataFrame = {\n    var fn_ = seq_dat_fr\n                .join(wresult_, \"hourly\")\n                .join(pdf_, \"date\")\n                .groupBy($\"shift\").agg(first(\"pm2_5_value\").alias(\"pm2_5_value\"))\n                .orderBy(asc(\"shift\"))    \n    return fn_\n}\ndef get_labels(seq_dat_fr : DataFrame, wresult_ : DataFrame): DataFrame = {\n    var fn_ = seq_dat_fr\n                .join(wresult_, \"hourly\")\n                .groupBy($\"shift\").agg(first(\"pm2_5_value\").alias(\"pm2_5_value\"))\n                .orderBy(asc(\"shift\"))    \n    return fn_\n}\n// val fn = generate_data_file(df, norm_data, wnorm, wresult, pnorm, pdf)\n// fn.rdd.coalesc.saveAsTextFile(\"/home/duclv/data/full_norm\")","dateUpdated":"2018-03-04T17:28:05+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1515825075510_177187886","id":"20180113-153115_1413180669","dateCreated":"2018-01-13T03:31:15+0900","dateStarted":"2018-03-04T10:36:30+0900","dateFinished":"2018-03-04T10:36:39+0900","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:232"},{"text":"def merge_china(vectors: DataFrame, data: DataFrame, feature_alias: String = \"china\") : DataFrame = {\n    var v_ = generate_seq(vectors)\n    var d_ = generate_seq(data.orderBy(\"timestamp\"))\n    var final_w_frame = d_.join(v_, \"seq\")\n                              .select(d_(\"timestamp\"), $\"china\".alias(feature_alias))\n    return final_w_frame\n}","dateUpdated":"2018-03-04T17:17:31+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1520148578771_1648953366","id":"20180304-162938_1882136691","result":{"code":"SUCCESS","type":"TEXT","msg":"\nmerge_china: (vectors: org.apache.spark.sql.DataFrame, data: org.apache.spark.sql.DataFrame, feature_alias: String)org.apache.spark.sql.DataFrame\n"},"dateCreated":"2018-03-04T16:29:38+0900","dateStarted":"2018-03-04T17:17:31+0900","dateFinished":"2018-03-04T17:17:31+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:233"},{"text":"// process weather data\nval prefix = \"/home/duclv/data/weather_daeku_201706_12/\"\nval p_prefix = \"/home/duclv/data/pressure_dae_ku_2017_06_12/\"\n\nval schema = StructType(\n        StructField(\"code\", IntegerType, true) ::\n        StructField(\"location\", StringType, true) ::\n        StructField(\"hourly\", StringType, true) ::\n        StructField(\"temp\", DoubleType, true) ::\n        StructField(\"prec\", DoubleType, true) ::\n        StructField(\"wind_sp\", DoubleType, true) ::\n        StructField(\"wind_agl\", DoubleType, true) ::\n        StructField(\"wind_dir\", StringType, true) :: Nil\n    )\nval pschema = StructType(\n        StructField(\"code\", StringType, true) ::\n        StructField(\"location\", StringType, true) ::\n        StructField(\"date\", StringType, true) ::\n        StructField(\"pres_value\", DoubleType, true) :: Nil\n    )\n    \nval weathers = spark.read.option(\"header\", \"false\").schema(schema).csv(prefix + \"weathers.csv\")\nval pressures = spark.read.option(\"header\", \"false\").schema(pschema).csv(p_prefix + \"pressures.csv\")","dateUpdated":"2018-03-04T18:28:30+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1516262659878_1423472244","id":"20180118-170419_2028118262","dateCreated":"2018-01-18T05:04:19+0900","dateStarted":"2018-03-03T14:57:55+0900","dateFinished":"2018-03-03T14:57:57+0900","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:234"},{"text":"def process_china(df: DataFrame) : DataFrame = {\n    var df_ = df.withColumn(\"timestamp\", udf_datetime(df(\"day\"), df(\"hour\")))\n    var mean_d = df_.groupBy(\"day\").agg(avg(\"avg\").alias(\"mean_pm25\"))\n    df_ = df_.join(full_date, full_date(\"day\") === df_(\"timestamp\"), \"right_outer\")\n             .select(full_date(\"day\").alias(\"timestamp\"), $\"avg\")\n             .withColumn(\"timestamp\", unix_timestamp($\"timestamp\", \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\n             .withColumn(\"day\", to_date($\"timestamp\"))\n    var f_df_ = df_.join(mean_d, df_(\"day\") === mean_d(\"day\"), \"left_outer\")\n                  .select($\"timestamp\", nanvl($\"avg\", $\"mean_pm25\").alias(\"avg\"))\n                  .na.fill(0.0, Array(\"avg\"))\n    return f_df_\n}","dateUpdated":"2018-03-04T18:11:05+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1520042265550_-813402458","id":"20180303-105745_785585684","result":{"code":"SUCCESS","type":"TEXT","msg":"\nprocess_china: (df: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"},"dateCreated":"2018-03-03T10:57:45+0900","dateStarted":"2018-03-04T18:11:05+0900","dateFinished":"2018-03-04T18:11:05+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:235"},{"text":"var beij = spark.read.option(\"header\", \"true\").schema(cschema).csv(c_prefix + \"beijing.csv\")\nvar shen = spark.read.option(\"header\", \"true\").schema(cschema).csv(c_prefix + \"shenyang.csv\")","dateUpdated":"2018-03-04T18:13:57+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1519992509756_1023669707","id":"20180302-210829_1626654725","result":{"code":"SUCCESS","type":"TEXT","msg":"\nbeij: org.apache.spark.sql.DataFrame = [day: string, hour: int ... 3 more fields]\n\nshen: org.apache.spark.sql.DataFrame = [day: string, hour: int ... 3 more fields]\n"},"dateCreated":"2018-03-02T21:08:29+0900","dateStarted":"2018-03-04T17:58:33+0900","dateFinished":"2018-03-04T17:58:34+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:236"},{"text":"var beijing = process_china(beij)\nvar shenyang = process_china(shen)\n\nvar (b_vectors, b_sc) = normalize_vector(beijing, cn_features, \"china\", \"timestamp\")\nvar (s_vectors, s_sc) = normalize_vector(shenyang, cn_features, \"china\", \"timestamp\")","dateUpdated":"2018-03-04T18:13:49+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1520152428124_-30142631","id":"20180304-173348_196659218","result":{"code":"SUCCESS","type":"TEXT","msg":"\nbeijing: org.apache.spark.sql.DataFrame = [timestamp: timestamp, avg: double]\n\nshenyang: org.apache.spark.sql.DataFrame = [timestamp: timestamp, avg: double]\n\n\nb_vectors: org.apache.spark.sql.DataFrame = [china: vector, timestamp: timestamp]\nb_sc: org.apache.spark.ml.feature.MinMaxScalerModel = minMaxScal_a80fcff8d5b7\n\n\ns_vectors: org.apache.spark.sql.DataFrame = [china: vector, timestamp: timestamp]\ns_sc: org.apache.spark.ml.feature.MinMaxScalerModel = minMaxScal_f5e2ad44b238\n"},"dateCreated":"2018-03-04T17:33:48+0900","dateStarted":"2018-03-04T18:13:49+0900","dateFinished":"2018-03-04T18:13:53+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:237"},{"text":"// weather and pressure from 06 -> 01 2018\nvar wresult_f = init_weather(weathers)\nvar pdf_f = init_pressure(pressures)\n// test weather and pressure","dateUpdated":"2018-03-02T15:11:01+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1516687230480_1623423315","id":"20180123-150030_1522173673","result":{"code":"SUCCESS","type":"TEXT","msg":"\nwresult_f: org.apache.spark.sql.DataFrame = [hourly: timestamp, temp: double ... 4 more fields]\n\npdf_f: org.apache.spark.sql.DataFrame = [date: date, pres_value: double]\n"},"dateCreated":"2018-01-23T03:00:30+0900","dateStarted":"2018-03-02T15:11:01+0900","dateFinished":"2018-03-02T15:11:02+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:238"},{"text":"// 327177\n//create vector for pressure\nvar (pnorm_f, pressure_scaler) = normalize_vector(pdf_f, features_pressure, \"pres_feature\", \"date\") // create vector for weather\n//create vector for weather\n// var (wnorm_f, weather_scaler) = normalize_vector(wresult_f.filter($\"temp\".isNotNull && $\"prec\".isNotNull && $\"wind_sp\".isNotNull), features_weather, \"weather_feature\", \"hourly\")\n// val pnorm_f = maxmin_vector(pdf_f, features_pressure, \"pres_feature\", \"date\")\nvar (wnorm_f, weather_scaler) = normalize_vector(wresult_f.filter($\"temp\".isNotNull && $\"prec\".isNotNull && $\"wind_sp\".isNotNull), features_weather, \"weather_feature\", \"hourly\")","dateUpdated":"2018-03-04T10:28:44+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1516678474714_-1356976220","id":"20180123-123434_1865848741","dateCreated":"2018-01-23T12:34:34+0900","dateStarted":"2018-03-04T10:28:44+0900","dateFinished":"2018-03-04T10:28:47+0900","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:239"},{"text":"// daegu data 06 -> 01 2018\nvar df_merge = get_core_df(datap, false)\ndf_merge.createOrReplaceTempView(\"sensors_fined\")\n// 327177\n// 330391\nvar (norm_data_merge, train_scaler) = normalize_vector(df_merge, features_common, \"common_feature\")","dateUpdated":"2018-03-04T10:30:22+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1515978502211_1932457760","id":"20180115-100822_67971903","dateCreated":"2018-01-15T10:08:22+0900","dateStarted":"2018-03-04T10:30:22+0900","dateFinished":"2018-03-04T10:31:06+0900","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:240"},{"text":"var labels = get_labels(df_merge, wresult_f, pdf_f)\nlabels.rdd.coalesce(1).saveAsTextFile(\"/home/duclv/data/labels\")","dateUpdated":"2018-01-25T07:25:26+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1516871234055_-116723569","id":"20180125-180714_1477958712","dateCreated":"2018-01-25T06:07:14+0900","dateStarted":"2018-01-25T07:23:10+0900","dateFinished":"2018-01-25T07:23:44+0900","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:241"},{"text":"// var fn_train_m = generate_data_file(df_merge, norm_data_merge, wnorm_f, wresult_f, pnorm_f, pdf_f)\nvar fn_train_m = generate_data_file(df_merge, norm_data_merge, wnorm_f, wresult_f, pnorm_f, pdf_f, b_vectors, beijing, s_vectors, shenyang)\n// fn_train_m.rdd.coalesce(1).saveAsTextFile(\"/home/duclv/data/full_norm_f\")","dateUpdated":"2018-03-04T18:14:05+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1516863423352_-1193373511","id":"20180125-155703_221564812","result":{"code":"SUCCESS","type":"TEXT","msg":"\nfn_train_m: org.apache.spark.sql.DataFrame = [shift: timestamp, collect_list(common_feature): array<vector> ... 5 more fields]\n"},"dateCreated":"2018-01-25T03:57:03+0900","dateStarted":"2018-03-04T18:14:05+0900","dateFinished":"2018-03-04T18:14:58+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:242"},{"text":"// testing data\nval data_test = spark.read.option(\"header\", \"true\").schema(dg_schema).csv(\"/home/duclv/data/test.csv\")\nvar df_test = get_core_df(data_test, true)","dateUpdated":"2018-03-04T18:31:45+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1516945713173_-1482774121","id":"20180126-144833_2058483272","result":{"code":"SUCCESS","type":"TEXT","msg":"\ndata_test: org.apache.spark.sql.DataFrame = [id: string, gateway_id: string ... 42 more fields]\n\ndf_test: org.apache.spark.sql.DataFrame = [timestamp: timestamp, lat: double ... 13 more fields]\n"},"dateCreated":"2018-01-26T02:48:33+0900","dateStarted":"2018-03-04T18:31:45+0900","dateFinished":"2018-03-04T18:31:46+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:243"},{"text":"var norm_test = normalize_vector_to_trainset(train_scaler, df_test, features_common, \"common_feature\")","dateUpdated":"2018-02-08T17:12:14+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1516947487865_-2003999967","id":"20180126-151807_97191542","result":{"code":"SUCCESS","type":"TEXT","msg":"\nnorm_test: org.apache.spark.sql.DataFrame = [timestamp: timestamp, common_feature: vector]\n"},"dateCreated":"2018-01-26T03:18:07+0900","dateStarted":"2018-02-08T17:12:14+0900","dateFinished":"2018-02-08T17:12:15+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:244"},{"text":"// min-max 06 -> 12\nvar max_val = df_merge.select(max($\"pm2_5_value\"), max($\"pm10_value\"), max($\"lat\"), max($\"lng\"), max($\"voc_value\"), \n        max($\"so2_value\"), max($\"no2_value\"), max($\"co_value\"), max($\"mcp_value\"), max($\"dow\"), max($\"hour\")).first()\nvar min_val = df_merge.select(min($\"pm2_5_value\"), min($\"pm10_value\"), min($\"lat\"), min($\"lng\"), min($\"voc_value\"), \n                        min($\"so2_value\"), min($\"no2_value\"), min($\"co_value\"), min($\"mcp_value\"), min($\"dow\"), min($\"hour\")).first()\n// var max_val_pr = pdf_f.select(max($\"pres_value\")).first()\n// var min_val_pr = pdf_f.select(min($\"pres_value\")).first()\n// var max_val_w = wresult_f.select(max($\"temp\"), max($\"prec\"), max($\"wind_sp\")).first()\n// var min_val_w = wresult_f.select(min($\"temp\"), min($\"prec\"), min($\"wind_sp\")).first()","dateUpdated":"2018-03-05T17:03:17+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1516511523278_1852753674","id":"20180121-141203_1534353371","result":{"code":"SUCCESS","type":"TEXT","msg":"\nmax_val: org.apache.spark.sql.Row = [222.58823529411765,276.3529411764706,35.980583833333334,128.7313831666667,72.9000015258789,12.503479166761585,12.508875000174157,12.71250000782311,99.5999984741211,1,23]\n\nmin_val: org.apache.spark.sql.Row = [0.0,0.0,35.709916416666665,128.42544366666667,0.5142857347215924,0.0,0.0,0.0,0.0,0,0]\n\nmax_val_w: org.apache.spark.sql.Row = [37.15,58.5,6.375]\n\nmin_val_w: org.apache.spark.sql.Row = [-13.475000000000001,0.0,0.125]\n"},"dateCreated":"2018-01-21T02:12:03+0900","dateStarted":"2018-03-04T18:39:00+0900","dateFinished":"2018-03-04T18:40:03+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:245"},{"text":"// merge maxmin scaler to test data\nvar mvd = sc.parallelize(Seq(max_val, min_val))\n            .map(row => (\"2017-06-01 00:00:00\", row.getDouble(2), row.getDouble(3), row.getDouble(0), row.getDouble(1), row.getDouble(4), \n                        row.getDouble(5), row.getDouble(6), row.getDouble(7), row.getDouble(8), row.getInt(9), row.getInt(10)))\n            .toDF(\"timestamp\", \"lat\", \"lng\", \"pm2_5_value\", \"pm10_value\", \"voc_value\", \"no2_value\", \"so2_value\", \"co_value\", \"mcp_value\", \"hour\", \"dow\")\n            .withColumn(\"timestamp\", unix_timestamp($\"timestamp\", \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\n            .withColumn(\"shift\", unix_timestamp($\"timestamp\", \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\n            .withColumn(\"hourly\", unix_timestamp($\"timestamp\", \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\n            .withColumn(\"date\", to_date($\"timestamp\"))\n            .select($\"timestamp\",$\"lat\",$\"lng\",$\"pm2_5_value\",$\"pm10_value\",$\"voc_value\",$\"so2_value\",$\"no2_value\",$\"co_value\",$\"mcp_value\",$\"shift\",$\"hourly\",$\"hour\",$\"dow\",$\"date\")\nvar data_test_final = df_test.union(mvd).orderBy(asc(\"timestamp\"))\nvar (norm_test, scaler_t) = normalize_vector(data_test_final, features_common, \"common_feature\")","dateUpdated":"2018-03-05T10:19:33+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1516707669564_-1860626799","id":"20180123-204109_987262640","result":{"code":"SUCCESS","type":"TEXT","msg":"\nmvd: org.apache.spark.sql.DataFrame = [timestamp: timestamp, lat: double ... 13 more fields]\n\ndata_test_final: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [timestamp: timestamp, lat: double ... 13 more fields]\n\n\nnorm_test: org.apache.spark.sql.DataFrame = [common_feature: vector, timestamp: timestamp]\nscaler_t: org.apache.spark.ml.feature.MinMaxScalerModel = minMaxScal_f363d85852bd\n"},"dateCreated":"2018-01-23T08:41:09+0900","dateStarted":"2018-03-05T10:19:33+0900","dateFinished":"2018-03-05T10:19:46+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:246"},{"text":"var max_val_t = df_test.select(max($\"pm2_5_value\"), max($\"pm10_value\"), max($\"lat\"), max($\"lng\"), max($\"voc_value\"), \n        max($\"so2_value\"), max($\"no2_value\"), max($\"co_value\"), max($\"mcp_value\"), max($\"dow\"), max($\"hour\")).first()\nvar min_val_t = df_test.select(min($\"pm2_5_value\"), min($\"pm10_value\"), min($\"lat\"), min($\"lng\"), min($\"voc_value\"), \n                        min($\"so2_value\"), min($\"no2_value\"), min($\"co_value\"), min($\"mcp_value\"), min($\"dow\"), min($\"hour\")).first()","dateUpdated":"2018-03-05T17:04:36+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1520237031659_-1186263733","id":"20180305-170351_707842220","result":{"code":"SUCCESS","type":"TEXT","msg":"\nmax_val_t: org.apache.spark.sql.Row = [149.74285714285713,174.65714285714284,35.91378616666667,128.63893561111115,10.524590189339685,0.005083333351649344,0.015470588853692306,1.9359999853372574,78.36666531032986,1,23]\n\nmin_val_t: org.apache.spark.sql.Row = [2.911111111111111,4.021739130434782,35.80778115384615,128.4924272631579,0.6000000238418579,0.0026666667545214295,0.01283333357423544,0.0,62.78918776641021,0,0]\n"},"dateCreated":"2018-03-05T17:03:51+0900","dateStarted":"2018-03-05T17:04:36+0900","dateFinished":"2018-03-05T17:04:45+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:247"},{"text":"var fn_test = generate_data_file(data_test_final, norm_test, wnorm_f, wresult_f, pnorm_f, pdf_f, b_vectors, beijing, s_vectors, shenyang)\n// var fn_test = generate_data_file(df_test, norm_test, wnorm_f, wresult_f)\n// fn_test.rdd.coalesce(1).saveAsTextFile(\"/home/duclv/data/test_norm_2\")\n// fn_test.count()","dateUpdated":"2018-03-05T10:44:04+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1516965840568_-1953503666","id":"20180126-202400_1011378263","result":{"code":"SUCCESS","type":"TEXT","msg":"\nfn_test: org.apache.spark.sql.DataFrame = [shift: timestamp, collect_list(common_feature): array<vector> ... 5 more fields]\n"},"dateCreated":"2018-01-26T08:24:00+0900","dateStarted":"2018-03-05T10:44:04+0900","dateFinished":"2018-03-05T10:44:25+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:248"},{"text":"var labels_test = get_labels(df_test, wresult_f)\nlabels_test.rdd.coalesce(1).saveAsTextFile(\"/home/duclv/data/test_labels_china\")","dateUpdated":"2018-03-05T14:58:50+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1516707311683_37389310","id":"20180123-203511_298276351","result":{"code":"SUCCESS","type":"TEXT","msg":"\nlabels_test: org.apache.spark.sql.DataFrame = [shift: timestamp, pm2_5_value: double]\n"},"dateCreated":"2018-01-23T08:35:11+0900","dateStarted":"2018-03-05T14:58:50+0900","dateFinished":"2018-03-05T14:58:59+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:249"},{"text":"var mean = df_merge.agg(avg($\"pm2_5_value\"), avg($\"pm10_value\"), avg($\"lat\"), avg($\"lng\"), avg($\"voc_value\"), \n        avg($\"so2_value\"), avg($\"no2_value\"), avg($\"co_value\"), avg($\"mcp_value\")).first()\nvar std = df_merge.agg(avg(pow($\"pm2_5_value\", 2)), avg(pow($\"pm10_value\", 2)), avg(pow($\"lat\", 2)), avg(pow($\"lng\", 2)), avg(pow($\"voc_value\", 2)),                                     \n        avg(pow($\"so2_value\", 2)), avg(pow($\"no2_value\", 2)), avg(pow($\"co_value\", 2)), avg(pow($\"mcp_value\", 2))).first()\n","dateUpdated":"2018-03-05T16:25:07+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1517467897044_-1399713199","id":"20180201-155137_884912113","result":{"code":"SUCCESS","type":"TEXT","msg":"\nmean: org.apache.spark.sql.Row = [29.73645648082214,33.44946334245609,35.86513577083615,128.58230807767362,7.539803581144471,0.006096134239518917,0.0533148396623968,0.26761245006521095,74.24350412190547]\n"},"dateCreated":"2018-02-01T03:51:37+0900","dateStarted":"2018-02-02T02:24:35+0900","dateFinished":"2018-02-02T02:25:15+0900","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:250"},{"text":"","dateUpdated":"2018-02-02T03:58:54+0900","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1517554724125_874778408","id":"20180202-155844_662356772","dateCreated":"2018-02-02T03:58:44+0900","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:251"}],"name":"daegu_cuong","id":"2D4U98M63","angularObjects":{"2BW3BFX9A:shared_process":[],"2BYGWTFFS:shared_process":[],"2BWFKEP71:shared_process":[],"2BY47YD2R:shared_process":[],"2BW682JRZ:shared_process":[],"2BV3DJM5V:shared_process":[],"2BYUSWKWP:shared_process":[],"2BWZHHY58:shared_process":[],"2BXXKJ4E6:shared_process":[],"2BVCJXP9G:shared_process":[],"2BYCWN35W:shared_process":[],"2BXX6P1NT:shared_process":[],"2BV7X75SQ:shared_process":[],"2BVB2D5M5:shared_process":[],"2BYQ3SX7P:shared_process":[],"2BX18Z83U:shared_process":[],"2BXJ4QKP7:shared_process":[],"2BYUQ5U3Q:shared_process":[]},"config":{"looknfeel":"default"},"info":{}}